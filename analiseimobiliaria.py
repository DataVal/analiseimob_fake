# -*- coding: utf-8 -*-
"""analiseimobiliaria.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RoSNdjTFpNC7j-PAOWNk7kKoOqqMkje0

# Início da análise
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

############## CARREGANDO O DATASET ##############

df = pd.read_csv("/content/sample_data/dados_falsos_Imobiliário.csv")

############## OBSERVAÇÕES INICIAIS ##############

# DICA: Caso não queira ver toda informação de uma vez mas também não goste de
#       deixar o código todo comentado utilize um comando de exec. seleção


# Ver as primeiras linhas -> mostra primeiras linhas do dataframe
print(df.head())

# Ver informações gerais -> mostra os nomes de coluna, contagem de not null e o tipo da coluna
print(df.info())

# Estatísticas básicas -> mostra algumas estatísticas descritivas: count, media, desvio padrão, min, max e quartis
print(df.describe())

# Ver apenas os nomes das colunas -> gera um objeto com nome de todas colunas
print(df.columns)
print(list(df.columns))

"""# Tratando valores ausentes, removendo linhas e colunas"""

# Vou começar removendo CPF e Endereço por enquanto, já que eles não serão usados nessa análise
df.drop(columns=["CPF", "Endereço do Imóvel"], inplace=True)

# Vendo se existem valores nulos -> me dá uma tabelinha com valores nulos por coluna
print(df.isnull().sum())

# Removendo todas as linhas que contêm pelo menos UM valor nulo
df.dropna(inplace=True)

# Remove colunas com valores nulos
df.dropna(axis=1, inplace=True)

# Remove linhas onde 'coluna_específica' é nulo
df.dropna(subset=['coluna_específica'], inplace=True)

# Mas e se eu não quiser remover os valores nulos?
# Não é recomendado mas se insiste é só fazer preencher os valores ausentes com um valor padrão, como zero ou a média da coluna
df.fillna(0, inplace=True)  # Substitui NaN por 0
df.fillna(df.mean(), inplace=True)  # Substitui NaN pela média da coluna

# Para preencher valores em uma coluna específica
df["Valor do Imóvel"].fillna(df["Valor do Imóvel"].median(), inplace=True)  # Preenche com a mediana da coluna especificada
df["Valor do Imóvel"].fillna(df["Valor do Imóvel"].mean(), inplace=True) # Preenche com a média da coluna especificada
df["Valor do Imóvel"].fillna(0, inplace=True) # Preenche com zero

# E para variáveis categóricas? Podemos preencher com a moda ou outro valor a nossa escolha
df["Tipo de Imóvel"].fillna(df["Tipo de Imóvel"].mode()[0], inplace=True)  # Preenche com o valor mais comum
df["Tipo de Imóvel"].fillna("Desconhecido", inplace=True)  # Preenche com um valor específico

# Na verdade podemos preencher uma coluna com valores muito além de somente moda, média e mediana, podemos usar min, max e quartis
df["Data da Última Reforma"] = pd.to_datetime(df["Data da Última Reforma"])
df["Data da Última Reforma"].fillna(pd.Timestamp("2000-01-01"), inplace=True)  # Define uma data padrão
df["Data da Última Reforma"].fillna(df["Data da Última Reforma"].max(), inplace=True)  # Define a data máxima como padrão
df["Data da Última Reforma"].fillna(df["Data da Última Reforma"].min(), inplace=True)  # Define a data mínima como padrão
df["Data da Última Reforma"].fillna(df["Data da Última Reforma"].median(), inplace=True)  # Define a mediana como padrão
df["Data da Última Reforma"].fillna(df["Data da Última Reforma"].mean(), inplace=True)  # Define a média como padrão
df["Data da Última Reforma"].fillna(df["Data da Última Reforma"].mode()[0], inplace=True)  # Define a moda como padrão
df["Data da Última Reforma"].fillna(df["Data da Última Reforma"].quantile(0.5), inplace=True)  # Define o primeiro quartil como padrão
df["Data da Última Reforma"].fillna(df["Data da Última Reforma"].quantile(0.75), inplace=True)  # Define o terceiro quartil como padrão
df["Data da Última Reforma"].fillna(df["Data da Última Reforma"].quantile(0.25), inplace=True)  # Define o segundo quartil como padrão

"""# Mudando os tipos das colunas"""

## Vendo os tipos de dados das colunas -> dá uma tabelinha com coluna e o tipo do dado
print(df.dtypes)

# EXTRA: Como verificar o uso da memória antes e depois

# Verificar consumo de memória antes da conversão
memoria_antes = df.memory_usage(deep=True).sum() / 1024**2  # Convertendo para MB
print(f"Memória antes da otimização: {memoria_antes:.2f} MB")

# Aplicando as conversões
df["Área do Imóvel (m²)"] = df["Área do Imóvel (m²)"].astype("int32")
df["Valor do Imóvel"] = df["Valor do Imóvel"].astype("int32")
df["Número de Quartos"] = df["Número de Quartos"].astype("int16")
df["Número de Banheiros"] = df["Número de Banheiros"].astype("int16")
df["Valor do Condomínio"] = df["Valor do Condomínio"].astype("int32")
df["Ano de Construção"] = df["Ano de Construção"].astype("int16")

# Vou aplicar uma lógica com lambda function e listas pra agilizar as coisas
colunas_binarias = ["Tem Piscina?", "Tem Garagem?", "Tem Elevador?", "Condomínio Fechado?"]
df[colunas_binarias] = df[colunas_binarias].apply(lambda x: x.map({"Sim": True, "Não": False}))
df[colunas_binarias] = df[colunas_binarias].astype("bool")

df["Data da Última Reforma"] = pd.to_datetime(df["Data da Última Reforma"], errors="coerce")

colunas_categoricas = ["Tipo de Imóvel", "Tipo de Oferta"]
df[colunas_categoricas] = df[colunas_categoricas].astype("category")

# Verificar consumo de memória depois da conversão
memoria_depois = df.memory_usage(deep=True).sum() / 1024**2  # Convertendo para MB
print(f"Memória depois da otimização: {memoria_depois:.2f} MB")

# Percentual de economia que foi obtido com as mudanças
economia = ((memoria_antes - memoria_depois) / memoria_antes) * 100
print(f"Redução de memória: {economia:.2f}%")

"""# Extra: conversão de tipo de coluna em massa"""

# Aulinha de como fazer conversões de tipo de coluna em massa
# Basta que criemos uma lista com o nome exato das colunas e façamos uma reatribuição

colunas_int = [
    "Área do Imóvel (m²)", "Valor do Imóvel", "Número de Quartos",
    "Número de Banheiros", "Valor do Condomínio", "Ano de Construção"
]

df[colunas_int] = df[colunas_int].astype("int32")  # Ou "int16" se os valores forem pequenos

# Se quisermos fazer isso com colunas onde vamos converter para booleano basta fazer com lambda
colunas_binarias = ["Tem Piscina?", "Tem Garagem?", "Tem Elevador?", "Condomínio Fechado?"]
df[colunas_binarias] = df[colunas_binarias].apply(lambda x: x.map({"Sim": True, "Não": False}))
df[colunas_binarias] = df[colunas_binarias].astype("bool")

# Com categóricas
colunas_categoricas = ["Tipo de Imóvel", "Tipo de Oferta"]
df[colunas_categoricas] = df[colunas_categoricas].astype("category")

"""# Identificando Outliers"""

# Agora vamos identificar outliers no nosso dataset, vou cirar uma função que recebe como parametro o df e a coluna e retorna os outliers

def detectar_outliers_iqr(df, coluna):
    Q1 = df[coluna].quantile(0.25)  # Primeiro quartil (25%)
    Q3 = df[coluna].quantile(0.75)  # Terceiro quartil (75%)
    IQR = Q3 - Q1  # Intervalo interquartil

    limite_inferior = Q1 - 1.5 * IQR
    limite_superior = Q3 + 1.5 * IQR

    outliers = df[(df[coluna] < limite_inferior) | (df[coluna] > limite_superior)]

    print(f"Número de outliers na coluna {coluna}: {outliers.shape[0]}")
    return outliers

# Exemplo: detectar outliers no "Valor do Imóvel"
outliers_valor = detectar_outliers_iqr(df, "Valor do Imóvel")
print(outliers_valor)

# Outro jeito de identificar se existem outliers é simplesmente plotando isso em um gráfico
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 5))
sns.boxplot(x=df["Valor do Imóvel"])
plt.title("Boxplot do Valor do Imóvel")
plt.show()

# Também tem como ver os outliers comparando o seu Z-Score que mede quantos desvios padrão um valor está longe da média.
# Valores muito altos (exemplo: Z > 3 ou Z < -3) podem ser considerados outliers.
from scipy.stats import zscore

df["Z_Score"] = zscore(df["Valor do Imóvel"])
outliers_zscore = df[df["Z_Score"].abs() > 3]  # Valores maiores que 3 ou menores que -3
print(outliers_zscore)

"""# Iniciando a Análise Exploratória dos Dados (EDA)

### Analisando os preços por tipo de imóvel
"""

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Ajustar o tamanho dos gráficos
plt.figure(figsize=(12, 6))

# Boxplot do valor do imóvel por tipo
sns.boxplot(x="Tipo de Imóvel", y="Valor do Imóvel", data=df)
plt.xticks(rotation=45)
plt.title("Distribuição dos preços por tipo de imóvel")
plt.show()

# Gráfico de barras com a média dos preços por tipo de imóvel
plt.figure(figsize=(12, 6))
sns.barplot(x="Tipo de Imóvel", y="Valor do Imóvel", data=df, estimator=np.mean, errorbar=None)
plt.xticks(rotation=45)
plt.title("Média de preços por tipo de imóvel")
plt.show()

"""### Correlação entre variáveis"""

import numpy as np

# Criar um mapa de calor de correlação entre as variáveis
plt.figure(figsize=(10,6))
sns.heatmap(df.corr(numeric_only=True), annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Correlação entre variáveis")
plt.show()

# Scatterplot - Valor do imóvel x Área
plt.figure(figsize=(10,6))
sns.scatterplot(x="Área do Imóvel (m²)", y="Valor do Imóvel", data=df)
plt.title("Relação entre Área do Imóvel e Valor")
plt.show()

# Scatterplot - Valor do imóvel x Número de Quartos
plt.figure(figsize=(10,6))
sns.scatterplot(x="Número de Quartos", y="Valor do Imóvel", data=df)
plt.title("Relação entre Número de Quartos e Valor")
plt.show()

"""# Relatório Final
*   Percebi que não existem valores nulos, o que é muita sorte (só que não) já que utilizei um gerador de dados fictícios que não tinha opção disso.
*   Não existem valores nulos e a maioria dos tipos das colunas são object e int64, logo preciso rever os tipos dessas colunas e trocá-los pra tipos mais adequados, pra isso as estatísticas descritivas vão ajudar muito com as variáveis não-categóricas.
*   Antes de trocar os tipos, mesmo que não existam valores ausentes fiz uma descrição do que poderia ser feito caso houvessem valores que requerissem o preenchimento.
*   Agora para os tipos das colunas vamos analisar cada uma individualmente:

  1. CPF - como vai ser um identificador único permanece como object
  2. Endereço de Imóvel - novamente, como não pode ser categorizado fica object
  3. Tipo de Imóvel (variável categórica) - esse já pode ser category
  4. Área do Imóvel (m²) - pode ser int32 já que nenhum imóvel possui valor acima de 4 bytes (2 bilhões)
  5. Valor do Imóvel - mesma lógica da área, muito dificilmente algum imóvel passará de 2 bilhões de reais, logo fica sendo int32
  6. Número de quartos - Pequenos valores, logo fica int16
  7. Número de banheiros - Mesma lógica do número de quartos, logo fica int16
  8. Tem Piscina? - Como representa uma pergunta de SIM/NÃO será booleano (bool)
  9. Tem Garagem? - Mesma lógica do acima - bool
  10. Tem Elevador? - Mesma lógica do acima - bool
  11. Condomínio Fechado? - Mesma lógica - bool
  12. Valor do Condomínio - Aqui vou deixar como int32, vai que é um condomínio de luxo ou o valor é anual, né?
  13. Ano de Construção - Aqui pode ficar sendo do tipo datetime64 ou int16
  14. Tipo de Oferta - Aqui pode ser category já que temos poucas opções
  15. Data da Última Reforma - Como uma data completa deverá ser datetime64

*   Depois de otimizar os tipos das colunas percebi uma redução de memória de quase 70%, incrível né? Somente trocando os tipos.
*   Notei que não havia outliers
*   Percebi que uma EDA no caso desse dataset gerado artificialmente seria extreamente desnecessária, já que as distribuição são extremamente simétricas em todos os tipos, além das correlações serem óbvias demais como número de quartos x número de banheiros (dir. proporcional) ou área do imóvel x preço do imóvel (mesma coisa).
*   No fim o exercício foi interessante pois aprendi bastante de como o pandas funciona, assim como outras bibliotecas como Seaborn, Numpy, Matplotlib e Scipy. Além disso pude ter uma boa noção de como seria um ETL em um caso normal. Para todos efeitos na próxima não utilizarei nenhum tipo de gerador de dados.

* PS: Mesmo tendo a limitação do dataset, agradeço a disponibilização do mesmo pelo site https://gerador-dados-falsos.streamlit.app criado pelo  Daniel Castro. Todos créditos ao Daniel que criou essa ferramenta para ajudar na criação de datasets.



"""